{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\llms\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "Answer: What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is\n",
      "\n",
      "Question: Who wrote 'Pride and Prejudice'?\n",
      "Answer: Who wrote 'Pride and Prejudice'?\n",
      "\n",
      "\"I'm not sure what to make of this. I think it's a bit of a stretch to say that it's a bit of a stretch to say that it's a bit of\n",
      "\n",
      "Question: How does photosynthesis work?\n",
      "Answer: How does photosynthesis work?\n",
      "\n",
      "Photochemists have long known that photosynthesis is a process of photosynthesis, but it is not clear how photosynthesis works.\n",
      "\n",
      "The photosynthetic process involves photosynthesis of the sugars in the food\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote 'Pride and Prejudice'?\",\n",
    "    \"How does photosynthesis work?\"\n",
    "]\n",
    "\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "for question in questions:\n",
    "    input_ids = tokenizer.encode(question, return_tensors='pt')\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, max_length=50, pad_token_id=pad_token_id)\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France?\n",
      "Answer: What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who wrote 'Pride and Prejudice'?\n",
      "Answer: Who wrote 'Pride and Prejudice'?\n",
      "\n",
      "\"I'm not sure what to make of this. I think it's a bit of a stretch to say that it's a bit of a stretch to say that it's a bit of\n",
      "\n",
      "Question: How does photosynthesis work?\n",
      "Answer: How does photosynthesis work?\n",
      "\n",
      "Photochemists have long known that photosynthesis is a process of photosynthesis, but it is not clear how photosynthesis works.\n",
      "\n",
      "The photosynthetic process involves photosynthesis of the sugars in the food\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote 'Pride and Prejudice'?\",\n",
    "    \"How does photosynthesis work?\"\n",
    "]\n",
    "\n",
    "\n",
    "for question in questions:\n",
    "    input_ids = tokenizer.encode(question, return_tensors='pt')\n",
    "\n",
    "\n",
    "    output = model.generate(input_ids, max_length=50)\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Good morning\n",
      " ### Answer: I'm listening. What do you want? (Intoxicated and heavily medicated) I don't think it's normal that you would need to make a situation for yourself. I've found that when we need to feel vulnerable to our surroundings, we are more drawn to the places in which we feel most comfortable. If the world around us is also hostile or oppressive, you may want to consider being more aware of that.If you're ready to\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "model_name = \"./test/checkpoint-3261\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "conversation_history = \"### Question: Good morning\\n ### Answer:\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer.encode(conversation_history, return_tensors='pt')\n",
    "\n",
    "\n",
    "output = model.generate(input_ids, max_length=100, do_sample=True, top_k=50)\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
